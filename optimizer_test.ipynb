{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./input_data\\train-images-idx3-ubyte.gz\n",
      "Extracting ./input_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting ./input_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./input_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('./input_data', one_hot=True, validation_size=500)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "batchSamples, batchLabels = mnist.train.next_batch(BATCH_SIZE)\n",
    "batchSamples = batchSamples.reshape((BATCH_SIZE, 28, 28, 1))\n",
    "batchSamples = np.pad(batchSamples, ((0,0),(2,2),(2,2),(0,0)), 'constant', constant_values=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "inTensor = batchSamples\n",
    "assert inTensor is batchSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "第一层为卷积层, 卷积层_1的参数  \n",
    "input:  filter: \n",
    "        size: 5*5;  number: 6  channel:  1  \n",
    "output: feature map: \n",
    "        size 28*28, number 6; channel :1\n",
    "\"\"\"\n",
    "CONV1_FILTER_SIZE = 5\n",
    "CONV1_FILTER_CHANNEL = 1\n",
    "CONV1_FILTER_NUM = 6\n",
    "\n",
    "\"\"\"\n",
    "第三层为卷积层, 卷积层_3的参数\n",
    "input: filter:\n",
    "       size 5 * 5   number 16 channel: 1\n",
    "output: feature map\n",
    "       size 10 * 10 number 16  channel: 1 \n",
    "       \n",
    "\"\"\"\n",
    "CONV3_FILTER_SIZE = 5\n",
    "CONV3_FILTER_CHANNEL = 1\n",
    "CONV3_FILTER_NUM = 16\n",
    "\n",
    "\"\"\"\n",
    "第五层是一个卷积层, \n",
    "input: filter:\n",
    "        size 5 *5    number 120   channel: 1\n",
    "output: feature map\n",
    "        size 1    number 120  channel: 1\n",
    "\"\"\"\n",
    "CONV5_FILTER_SIZE = 5\n",
    "CONV5_FILTER_CHANNEL = 1\n",
    "CONV5_FILTER_NUM = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\"\"\"conv 1.  第一层为卷积层,有6个filter\n",
    "input size: 32 * 32 * 1\n",
    "output size: 28 * 28 * 6\n",
    "\"\"\"\n",
    "conv1_filter = tf.Variable(tf.truncated_normal([CONV1_FILTER_SIZE,CONV1_FILTER_SIZE,CONV1_FILTER_CHANNEL, CONV1_FILTER_NUM],dtype=tf.float32))\n",
    "conv1 = tf.nn.conv2d(inTensor, conv1_filter, strides=[1, 1, 1, 1], padding='VALID')\n",
    "conv1_bias = tf.Variable(tf.truncated_normal([CONV1_FILTER_NUM], dtype=tf.float32))\n",
    "relu_1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n",
    "\n",
    "\"\"\"pooling 2.  \n",
    " 第2层为pooling层,与原始LetNet不同.原始的LeNet的pooling层有两个可训练参数\n",
    " input size: 28 * 28 * 6\n",
    " output size: 14 * 14 * 6\n",
    "\"\"\"\n",
    "pooling2 = tf.nn.avg_pool(relu_1, ksize=[1,2,2,1], strides=[1, 2, 2 ,1], padding=\"VALID\")\n",
    "relu_2 = tf.nn.relu(pooling2)\n",
    "\n",
    "\n",
    "\"\"\" conv 3  \n",
    "第3层为卷积层,共16个filter. 与原始LeNet不同,原始的LeNet是部分连接\n",
    "这里为了方便是,整体连接,详细情况见论文希捷\n",
    "input size 14 * 14 * 6\n",
    "out putsize 10 * 10 *16\n",
    "\"\"\"\n",
    "conv3_filter = tf.Variable(tf.truncated_normal([5, 5, 6, 16]))\n",
    "conv3_bias = tf.Variable(tf.truncated_normal([16]))\n",
    "conv3 = tf.nn.conv2d(relu_2, conv3_filter, strides=[1,1,1,1],padding='VALID')\n",
    "relu_3 =tf.nn.relu(tf.nn.bias_add(conv3, conv3_bias))\n",
    "\n",
    "\n",
    "\"\"\"pooling 4  \n",
    "第4层为pooling层, 与pooling 2 相同\n",
    "input size:  10 * 10 * 16\n",
    "output size: 5 * 5 * 16\n",
    "\"\"\"\n",
    "pooling4 = tf.nn.avg_pool(relu_3, ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"VALID\")\n",
    "relu_4 = tf.nn.relu(pooling4)\n",
    "\n",
    "\"\"\"conv 5\n",
    "第5层为conv层, 这一层与原始的LeNet-5是一致的\n",
    "input size: 5 * 5 * 16\n",
    "output size: 1 * 1 * 120\n",
    "\"\"\"\n",
    "conv5_filter = tf.Variable(tf.truncated_normal([5, 5, 16, 120]))\n",
    "conv5_bias = tf.Variable(tf.truncated_normal([120]))\n",
    "conv5 = tf.nn.conv2d(relu_4, conv5_filter, strides=[1,1,1,1],padding=\"VALID\")\n",
    "relu_5 = tf.nn.relu(tf.nn.bias_add(conv5, conv5_bias))\n",
    "\n",
    "\"\"\"fc 6\n",
    "第6层为fc层,与原始不太一致,因为我只在mnist上进性试验\n",
    "input size: BATCH_SIZE * 1 * 1 * 120\n",
    "output size: BATCH_SIZE * 1 * 1 * 10\n",
    "\"\"\"\n",
    "fc_weights = tf.Variable(tf.truncated_normal([120,10]))\n",
    "fc_bias = tf.Variable(tf.truncated_normal([10]))\n",
    "relu_5 = tf.reshape(relu_5, [BATCH_SIZE, 120])\n",
    "relu_6 = tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(relu_5, fc_weights),fc_bias))\n",
    "\n",
    "\n",
    "\"\"\"最后的输出单元\n",
    "softmax分类器\n",
    "\"\"\"\n",
    "logits = tf.nn.softmax(relu_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batchLabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=batchLabels, logits=relu_6 )\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "#train\n",
    "train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.44068\n",
      "[[ 0.45515513  0.87927562 -1.86986887 -0.4473561  -1.53186631]\n",
      " [ 0.76428258  0.89881247 -0.45456028  0.3070229   0.59747416]\n",
      " [-1.41233289  1.31793773 -0.0540248  -1.08194458  0.36511591]\n",
      " [ 1.33356881  0.38394856 -0.6336869  -0.03608479  1.14866793]\n",
      " [-0.82556474  0.20747866 -0.29158038  1.24992657  0.05491471]]\n",
      "[[ 0.45571333  0.87896752 -1.87000918 -0.44719428 -1.53188133]\n",
      " [ 0.76348394  0.89852756 -0.45353431  0.30837655  0.59813327]\n",
      " [-1.41404521  1.31715059 -0.05334353 -1.08123827  0.36528185]\n",
      " [ 1.33123732  0.38235626 -0.63454562 -0.03711052  1.14746463]\n",
      " [-0.82740611  0.20574446 -0.29353344  1.24770451  0.05291207]]\n",
      "2.45421\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(loss))\n",
    "    print(sess.run(conv1_filter[:,:,0,0]))\n",
    "    for i in range(1000):\n",
    "        sess.run(train_op)\n",
    "    print(sess.run(conv1_filter[:,:,0,0]))\n",
    "    print(sess.run(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = tf.Variable([[1,2,4]],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_samples, batch_labels=  mnist.train.next_batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 784)\n"
     ]
    }
   ],
   "source": [
    "print(batch_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " pad(array, pad_width, mode, **kwargs)\n",
      "\n",
      "Pads an array.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "array : array_like of rank N\n",
      "    Input array\n",
      "pad_width : {sequence, array_like, int}\n",
      "    Number of values padded to the edges of each axis.\n",
      "    ((before_1, after_1), ... (before_N, after_N)) unique pad widths\n",
      "    for each axis.\n",
      "    ((before, after),) yields same before and after pad for each axis.\n",
      "    (pad,) or int is a shortcut for before = after = pad width for all\n",
      "    axes.\n",
      "mode : str or function\n",
      "    One of the following string values or a user supplied function.\n",
      "\n",
      "    'constant'\n",
      "        Pads with a constant value.\n",
      "    'edge'\n",
      "        Pads with the edge values of array.\n",
      "    'linear_ramp'\n",
      "        Pads with the linear ramp between end_value and the\n",
      "        array edge value.\n",
      "    'maximum'\n",
      "        Pads with the maximum value of all or part of the\n",
      "        vector along each axis.\n",
      "    'mean'\n",
      "        Pads with the mean value of all or part of the\n",
      "        vector along each axis.\n",
      "    'median'\n",
      "        Pads with the median value of all or part of the\n",
      "        vector along each axis.\n",
      "    'minimum'\n",
      "        Pads with the minimum value of all or part of the\n",
      "        vector along each axis.\n",
      "    'reflect'\n",
      "        Pads with the reflection of the vector mirrored on\n",
      "        the first and last values of the vector along each\n",
      "        axis.\n",
      "    'symmetric'\n",
      "        Pads with the reflection of the vector mirrored\n",
      "        along the edge of the array.\n",
      "    'wrap'\n",
      "        Pads with the wrap of the vector along the axis.\n",
      "        The first values are used to pad the end and the\n",
      "        end values are used to pad the beginning.\n",
      "    <function>\n",
      "        Padding function, see Notes.\n",
      "stat_length : sequence or int, optional\n",
      "    Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of\n",
      "    values at edge of each axis used to calculate the statistic value.\n",
      "\n",
      "    ((before_1, after_1), ... (before_N, after_N)) unique statistic\n",
      "    lengths for each axis.\n",
      "\n",
      "    ((before, after),) yields same before and after statistic lengths\n",
      "    for each axis.\n",
      "\n",
      "    (stat_length,) or int is a shortcut for before = after = statistic\n",
      "    length for all axes.\n",
      "\n",
      "    Default is ``None``, to use the entire axis.\n",
      "constant_values : sequence or int, optional\n",
      "    Used in 'constant'.  The values to set the padded values for each\n",
      "    axis.\n",
      "\n",
      "    ((before_1, after_1), ... (before_N, after_N)) unique pad constants\n",
      "    for each axis.\n",
      "\n",
      "    ((before, after),) yields same before and after constants for each\n",
      "    axis.\n",
      "\n",
      "    (constant,) or int is a shortcut for before = after = constant for\n",
      "    all axes.\n",
      "\n",
      "    Default is 0.\n",
      "end_values : sequence or int, optional\n",
      "    Used in 'linear_ramp'.  The values used for the ending value of the\n",
      "    linear_ramp and that will form the edge of the padded array.\n",
      "\n",
      "    ((before_1, after_1), ... (before_N, after_N)) unique end values\n",
      "    for each axis.\n",
      "\n",
      "    ((before, after),) yields same before and after end values for each\n",
      "    axis.\n",
      "\n",
      "    (constant,) or int is a shortcut for before = after = end value for\n",
      "    all axes.\n",
      "\n",
      "    Default is 0.\n",
      "reflect_type : {'even', 'odd'}, optional\n",
      "    Used in 'reflect', and 'symmetric'.  The 'even' style is the\n",
      "    default with an unaltered reflection around the edge value.  For\n",
      "    the 'odd' style, the extented part of the array is created by\n",
      "    subtracting the reflected values from two times the edge value.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "pad : ndarray\n",
      "    Padded array of rank equal to `array` with shape increased\n",
      "    according to `pad_width`.\n",
      "\n",
      "Notes\n",
      "-----\n",
      ".. versionadded:: 1.7.0\n",
      "\n",
      "For an array with rank greater than 1, some of the padding of later\n",
      "axes is calculated from padding of previous axes.  This is easiest to\n",
      "think about with a rank 2 array where the corners of the padded array\n",
      "are calculated by using padded values from the first axis.\n",
      "\n",
      "The padding function, if used, should return a rank 1 array equal in\n",
      "length to the vector argument with padded values replaced. It has the\n",
      "following signature::\n",
      "\n",
      "    padding_func(vector, iaxis_pad_width, iaxis, **kwargs)\n",
      "\n",
      "where\n",
      "\n",
      "    vector : ndarray\n",
      "        A rank 1 array already padded with zeros.  Padded values are\n",
      "        vector[:pad_tuple[0]] and vector[-pad_tuple[1]:].\n",
      "    iaxis_pad_width : tuple\n",
      "        A 2-tuple of ints, iaxis_pad_width[0] represents the number of\n",
      "        values padded at the beginning of vector where\n",
      "        iaxis_pad_width[1] represents the number of values padded at\n",
      "        the end of vector.\n",
      "    iaxis : int\n",
      "        The axis currently being calculated.\n",
      "    kwargs : misc\n",
      "        Any keyword arguments the function requires.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> a = [1, 2, 3, 4, 5]\n",
      ">>> np.lib.pad(a, (2,3), 'constant', constant_values=(4, 6))\n",
      "array([4, 4, 1, 2, 3, 4, 5, 6, 6, 6])\n",
      "\n",
      ">>> np.lib.pad(a, (2, 3), 'edge')\n",
      "array([1, 1, 1, 2, 3, 4, 5, 5, 5, 5])\n",
      "\n",
      ">>> np.lib.pad(a, (2, 3), 'linear_ramp', end_values=(5, -4))\n",
      "array([ 5,  3,  1,  2,  3,  4,  5,  2, -1, -4])\n",
      "\n",
      ">>> np.lib.pad(a, (2,), 'maximum')\n",
      "array([5, 5, 1, 2, 3, 4, 5, 5, 5])\n",
      "\n",
      ">>> np.lib.pad(a, (2,), 'mean')\n",
      "array([3, 3, 1, 2, 3, 4, 5, 3, 3])\n",
      "\n",
      ">>> np.lib.pad(a, (2,), 'median')\n",
      "array([3, 3, 1, 2, 3, 4, 5, 3, 3])\n",
      "\n",
      ">>> a = [[1, 2], [3, 4]]\n",
      ">>> np.lib.pad(a, ((3, 2), (2, 3)), 'minimum')\n",
      "array([[1, 1, 1, 2, 1, 1, 1],\n",
      "       [1, 1, 1, 2, 1, 1, 1],\n",
      "       [1, 1, 1, 2, 1, 1, 1],\n",
      "       [1, 1, 1, 2, 1, 1, 1],\n",
      "       [3, 3, 3, 4, 3, 3, 3],\n",
      "       [1, 1, 1, 2, 1, 1, 1],\n",
      "       [1, 1, 1, 2, 1, 1, 1]])\n",
      "\n",
      ">>> a = [1, 2, 3, 4, 5]\n",
      ">>> np.lib.pad(a, (2, 3), 'reflect')\n",
      "array([3, 2, 1, 2, 3, 4, 5, 4, 3, 2])\n",
      "\n",
      ">>> np.lib.pad(a, (2, 3), 'reflect', reflect_type='odd')\n",
      "array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8])\n",
      "\n",
      ">>> np.lib.pad(a, (2, 3), 'symmetric')\n",
      "array([2, 1, 1, 2, 3, 4, 5, 5, 4, 3])\n",
      "\n",
      ">>> np.lib.pad(a, (2, 3), 'symmetric', reflect_type='odd')\n",
      "array([0, 1, 1, 2, 3, 4, 5, 5, 6, 7])\n",
      "\n",
      ">>> np.lib.pad(a, (2, 3), 'wrap')\n",
      "array([4, 5, 1, 2, 3, 4, 5, 1, 2, 3])\n",
      "\n",
      ">>> def padwithtens(vector, pad_width, iaxis, kwargs):\n",
      "...     vector[:pad_width[0]] = 10\n",
      "...     vector[-pad_width[1]:] = 10\n",
      "...     return vector\n",
      "\n",
      ">>> a = np.arange(6)\n",
      ">>> a = a.reshape((2, 3))\n",
      "\n",
      ">>> np.lib.pad(a, 2, padwithtens)\n",
      "array([[10, 10, 10, 10, 10, 10, 10],\n",
      "       [10, 10, 10, 10, 10, 10, 10],\n",
      "       [10, 10,  0,  1,  2, 10, 10],\n",
      "       [10, 10,  3,  4,  5, 10, 10],\n",
      "       [10, 10, 10, 10, 10, 10, 10],\n",
      "       [10, 10, 10, 10, 10, 10, 10]])\n"
     ]
    }
   ],
   "source": [
    "np.info(np.pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
