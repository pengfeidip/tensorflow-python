{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import  numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 相关子函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f_conv(input, kernel_size, strides, padding_method= \"SAME\", name = \"conv\"):\n",
    "    \"\"\"\n",
    "    @Brief: construting convolution layer   \n",
    "    @input: a tensor:[batch_size, width, height, channels]  \n",
    "    @kernel_size: [width, heigt, channels, depth]  \n",
    "    @stide: stride of convolution :[1, up-down, left-right, 1]\n",
    "    @name: name of this op, default is \"conv\"  \n",
    "    @padding_method: default is valid\n",
    "    @return : activations\n",
    "    \"\"\"\n",
    "\n",
    "    assert len(input.shape) == 4, \"input shape is illegal\"\n",
    "\n",
    "\n",
    "    with tf.name_scope(name):\n",
    "        filter = tf.Variable(tf.truncated_normal(shape = kernel_size, stddev=  0.1), name=\"weights\")\n",
    "        bias = tf.Variable(tf.truncated_normal(shape = [kernel_size[3]], stddev=  0.1), name=\"bias\")\n",
    "        #padding is valid is form the paper\n",
    "        conv = tf.nn.conv2d(input, filter, strides = strides, padding = padding_method, name=\"conv\")\n",
    "        relu = tf.nn.relu(conv+bias, name=\"relu\")\n",
    "    return relu\n",
    "\n",
    "def f_pooling(input, kernel_size, strides, method = \"max\", padding_method= \"VALID\", name = \"pooling\"):\n",
    "    \"\"\"\n",
    "    @ Brief: pooling layer  \n",
    "    @ input: a tensor has a shape of [batch_size, width, height, channels]  \n",
    "    @ kernel_size: a list with shape (4, ),: [width, heigt, channels, depth] \n",
    "    @ strides: pooling strides  \n",
    "    @ method: default is max-pooling . all: {\"max\", \"avg\"}  \n",
    "    @ padding_method: default is \"valid\"\n",
    "    @ return pooling result\n",
    "    \"\"\"\n",
    "    assert method == \"max\" or method == \"avg\", \"methond is illegal\"\n",
    "    with tf.name_scope(name):\n",
    "        if method == \"max\":\n",
    "            pooling = tf.nn.max_pool(input, ksize = kernel_size, strides=strides, padding = padding_method, name = \"max_pooling\")\n",
    "        elif method == \"avg\":\n",
    "            pooling = tf.nn.avg_pool(input, ksize = kernel_size, strides=strides, padding = padding_method, name = 'avg_pooling')\n",
    "    return pooling\n",
    "\n",
    "def f_fc(input, nodes,  name = \"fc\"):\n",
    "    \"\"\"\n",
    "    @ Brief:  constructing fully conncected layer:\n",
    "    @ input: a tensor with shape of [batch_size , width * height * channels] \n",
    "    @ nodes: nodes in this fully connected \n",
    "    @ name: the name you give the layer\n",
    "    @ @Note: you should reshape the input tensor , if last layer is conv-layer\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name):\n",
    "        weight = tf.Variable(tf.truncated_normal(shape = [input.get_shape().as_list()[1], nodes], stddev = 0.1), name = \"weights\")\n",
    "        bias = tf.Variable(tf.truncated_normal(shape = [nodes], stddev = 0.1), name = \"bias\")                         \n",
    "        fc = tf.nn.relu(tf.matmul(input, weight) + bias, name = \"relu\")\n",
    "    return fc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def Alexnet_infer(input, train_flag = False, regularizer_flag = False):\n",
    "    \"\"\"\n",
    "    layer 1: 卷积 + max pooling()\n",
    "    输入： 224*224*3\n",
    "    卷积输出： 55 * 55* 96 （padding：same）， 分布在两个卡上\n",
    "    LRB： 原文中有， 我没做，因为帮助不大， 不改变尺寸\n",
    "    池化输出： 27 * 27 * 96（padding： valid）\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"layer_1\"):\n",
    "    # first convolution layer , in original paper, this layer distribute in two GPU\n",
    "        conv_11 = f_conv(input, kernel_size = [11, 11, 3, 48], strides = [1, 4, 4, 1], name = \"conv_1\")\n",
    "        conv_12 = f_conv(input, kernel_size = [11, 11, 3, 48], strides = [1, 4, 4, 1], name = \"conv_2\")\n",
    "        pooling_11 = f_pooling(conv_11, kernel_size = [1,3, 3, 1], strides = [1, 2, 2, 1], \n",
    "                            method = \"max\", padding_method=\"VALID\", name=\"pooling_1\")\n",
    "        pooling_12 = f_pooling(conv_12, kernel_size = [1,3, 3, 1], strides = [1, 2, 2, 1], \n",
    "                            method = \"max\", padding_method=\"VALID\", name=\"pooling_2\")\n",
    "    \"\"\"\n",
    "    layer 2: 卷积 + max pooling()\n",
    "    输入： 27 * 27 * 96(48*2), kerner_size:5 * 5 * 48 * 256(128*2)\n",
    "    卷积输出：27 * 27 * 256(128*2)（stride: 1, padding: same）分布在两个卡上， 不交流\n",
    "    LRB： 原文中有， 我没做，因为帮助不大， 不改变尺寸\n",
    "    池化输出：13 * 13 * 256（padding： valid）\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"layer_2\"):\n",
    "        conv_21 = f_conv(pooling_11, [5, 5, 48, 128], strides=[1, 1, 1, 1],padding_method=\"SAME\", name=\"conv_1\")\n",
    "        conv_22 = f_conv(pooling_12, [5, 5, 48, 128], strides=[1, 1, 1, 1], padding_method=\"SAME\", name=\"conv_2\")\n",
    "        pooling_21 = f_pooling(conv_21, [1, 3, 3, 1], strides = [1, 2, 2, 1], \n",
    "                            method = \"max\", padding_method=\"VALID\", name=\"pooling_1\")\n",
    "        pooling_22 = f_pooling(conv_22, [1, 3, 3, 1], strides = [1, 2, 2, 1], \n",
    "                            method = \"max\", padding_method=\"VALID\", name=\"pooling_2\")\n",
    "\n",
    "    \"\"\"\n",
    "    layer 3: 卷积，双卡交流, 先把上面两个输出连接起来\n",
    "    输入：13 * 13 * 256\n",
    "    卷积输出：13 *13 * 384（padding：same），双卡交流\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"layer_3\"):\n",
    "        conv_31 = f_conv(tf.concat([pooling_21, pooling_22], axis = 3),\n",
    "                kernel_size = [3, 3, 256, 192],strides = [1,1,1,1], padding_method=\"SAME\", name = \"conv_1\")\n",
    "        conv_32= f_conv(tf.concat([pooling_21, pooling_22], axis = 3),\n",
    "                            kernel_size = [3, 3, 256, 192],strides = [1,1,1,1], padding_method=\"SAME\", name = \"conv_2\")\n",
    "\n",
    "    \"\"\"\n",
    "    layer 4: 卷积， 显卡之间不交流\n",
    "    输入：13 *13 * 384\n",
    "    卷积输出：13 *13 * 384（padding：same， stride：1），\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"layer_4\"):\n",
    "        conv_41 = f_conv(conv_31, kernel_size = [3, 3, 192, 192], \n",
    "                           strides = [1, 1, 1, 1], padding_method=\"SAME\", name = \"conv_1\")\n",
    "        conv_42 = f_conv(conv_32, kernel_size = [3, 3, 192, 192], \n",
    "                                        strides = [1, 1, 1, 1], padding_method=\"SAME\", name = \"conv_2\")\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    layer 5: 卷积 + pooling， 显卡之间不交流\n",
    "    输入：13 *13 * 384(192*2)（分布在两个显卡）\n",
    "    卷积输出 13 *13 * 256(128*2)（分布在两个显卡 padding same, stride 1）\n",
    "    池化输出： 6 * 6 * 256(128*2)（分布在两个显卡上，ksize：3，stride：2， padding：valid）\n",
    "    \"\"\"\n",
    "    with  tf.name_scope(\"layer_5\"):\n",
    "        conv_51 = f_conv(conv_41, kernel_size = [3, 3, 192, 128],\n",
    "                                strides = [1, 1, 1, 1], padding_method=\"SAME\", name = \"conv_1\")\n",
    "        conv_52 = f_conv(conv_42, kernel_size = [3, 3, 192, 128],\n",
    "                                strides = [1, 1, 1, 1], padding_method=\"SAME\", name = \"conv_2\")   \n",
    "        pooling_51 = f_pooling(conv_51, kernel_size = [1, 3, 3, 1], strides = [1, 2, 2, 1],\n",
    "                                 padding_method = \"VALID\", name=\"pooling_1\")\n",
    "        pooling_52 = f_pooling(conv_52, kernel_size = [1, 3, 3, 1], strides = [1, 2, 2, 1],\n",
    "                                    padding_method = \"VALID\", name=\"pooling_2\")\n",
    "\n",
    "    \"\"\"\n",
    "    layer 6: 全连接层 4096个单元， \n",
    "    这里是分开的， 每个显卡各2048， 我觉得没有必要 直接写成一起的\n",
    "    输入  6 * 6 * 256(128*2)\n",
    "    输出 4096\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"layer_6\"):\n",
    "        input_shape = tf.concat([pooling_51, pooling_52], axis=3).get_shape().as_list()\n",
    "        input_shaped = tf.reshape(tf.concat([pooling_51, pooling_52], axis=3), \n",
    "                                shape = [input_shape[0], input_shape[1]*input_shape[2]*input_shape[3]], name=\"reshape_input\")\n",
    "        if train_flag and regularizer_flag:\n",
    "            fc_6 = tf.nn.dropout(f_fc(input_shaped, 4096), keep_prob = 0.5, name = \"fc_dropout\")\n",
    "        else:\n",
    "            fc_6 = f_fc(input_shaped, 4096, name = \"fc\")\n",
    "            \n",
    "         \n",
    "\n",
    "    \"\"\"\n",
    "    layer 7: 全连接层 4096个单元， \n",
    "    这里是分开的， 每个显卡各2048， 我觉得没有必要 直接写成一起的\n",
    "    输入  4096\n",
    "    输出 4096\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"layer_7\"):\n",
    "        if train_flag and regularizer_flag:\n",
    "            fc_7 = tf.nn.dropout(f_fc(fc_6, 4096), keep_prob = 0.5, name = \"fc_dropout\")\n",
    "        else:\n",
    "            fc_7 = f_fc(fc_6, 4096, name = \"fc\")\n",
    "\n",
    "    \"\"\"\n",
    "    layer 8: 全连接层 1000 个单元， \n",
    "    输入  4096\n",
    "    输出 1000, 也就是每一类的概率\n",
    "    \"\"\" \n",
    "    with tf.name_scope(\"layer_8\"):\n",
    "        fc_8 = f_fc(fc_7, 1000, name = \"fc\" )\n",
    "\n",
    "\n",
    "    return fc_8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
