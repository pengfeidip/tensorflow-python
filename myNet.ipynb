{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import Ipynb_importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-05ab45eebe5b>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From G:\\Anaconda\\installCatalog\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From G:\\Anaconda\\installCatalog\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./input_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From G:\\Anaconda\\installCatalog\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./input_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From G:\\Anaconda\\installCatalog\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./input_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./input_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From G:\\Anaconda\\installCatalog\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./input_data', one_hot=True, validation_size=500)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# network 设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "第一层为卷积层, 卷积层_1的参数  \n",
    "input:  filter: \n",
    "        size: 5*5;  number: 6  channel:  1  \n",
    "output: feature map: \n",
    "        size 28*28, number 6; channel :1\n",
    "\"\"\"\n",
    "CONV1_FILTER_SIZE = 5\n",
    "CONV1_FILTER_CHANNEL = 1\n",
    "CONV1_FILTER_NUM = 6\n",
    "\n",
    "\"\"\"\n",
    "第三层为卷积层, 卷积层_3的参数\n",
    "input: filter:\n",
    "       size 5 * 5   number 16 channel: 1\n",
    "output: feature map\n",
    "       size 10 * 10 number 16  channel: 1 \n",
    "       \n",
    "\"\"\"\n",
    "CONV3_FILTER_SIZE = 5\n",
    "CONV3_FILTER_CHANNEL = 1\n",
    "CONV3_FILTER_NUM = 16\n",
    "\n",
    "\"\"\"\n",
    "第五层是一个卷积层, \n",
    "input: filter:\n",
    "        size 5 *5    number 120   channel: 1\n",
    "output: feature map\n",
    "        size 1    number 120  channel: 1\n",
    "\"\"\"\n",
    "CONV5_FILTER_SIZE = 5\n",
    "CONV5_FILTER_CHANNEL = 1\n",
    "CONV5_FILTER_NUM = 120\n",
    "\n",
    "\"\"\"\n",
    "设置一些训练参数\n",
    "\"\"\"\n",
    "BATCH_SIZE = 100;\n",
    "\n",
    "# LeNet_inference\n",
    "\n",
    "def LeNet_inference(inTensor):\n",
    "    \n",
    "    \"\"\"conv 1.  第一层为卷积层,有6个filter\n",
    "    input size: 32 * 32 * 1\n",
    "    output size: 28 * 28 * 6\n",
    "    \"\"\"\n",
    "    conv1_filter = tf.Variable(tf.truncated_normal([CONV1_FILTER_SIZE,CONV1_FILTER_SIZE,CONV1_FILTER_CHANNEL, CONV1_FILTER_NUM],dtype=tf.float32))\n",
    "    conv1 = tf.nn.conv2d(inTensor, conv1_filter, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    conv1_bias = tf.Variable(tf.truncated_normal([CONV1_FILTER_NUM], dtype=tf.float32))\n",
    "    relu_1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n",
    "    \n",
    "    \"\"\"pooling 2.  \n",
    "     第2层为pooling层,与原始LetNet不同.原始的LeNet的pooling层有两个可训练参数\n",
    "     input size: 28 * 28 * 6\n",
    "     output size: 14 * 14 * 6\n",
    "    \"\"\"\n",
    "    pooling2 = tf.nn.avg_pool(relu_1, ksize=[1,2,2,1], strides=[1, 2, 2 ,1], padding=\"VALID\")\n",
    "    relu_2 = tf.nn.relu(pooling2)\n",
    "    \n",
    "    \n",
    "    \"\"\" conv 3  \n",
    "    第3层为卷积层,共16个filter. 与原始LeNet不同,原始的LeNet是部分连接\n",
    "    这里为了方便是,整体连接,详细情况见论文希捷\n",
    "    input size 14 * 14 * 6\n",
    "    out putsize 10 * 10 *16\n",
    "    \"\"\"\n",
    "    conv3_filter = tf.Variable(tf.truncated_normal([5, 5, 6, 16]))\n",
    "    conv3_bias = tf.Variable(tf.truncated_normal([16]))\n",
    "    conv3 = tf.nn.conv2d(relu_2, conv3_filter, strides=[1,1,1,1],padding='VALID')\n",
    "    relu_3 =tf.nn.relu(tf.nn.bias_add(conv3, conv3_bias))\n",
    "    \n",
    "    \n",
    "    \"\"\"pooling 4  \n",
    "    第4层为pooling层, 与pooling 2 相同\n",
    "    input size:  10 * 10 * 16\n",
    "    output size: 5 * 5 * 16\n",
    "    \"\"\"\n",
    "    pooling4 = tf.nn.avg_pool(relu_3, ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"VALID\")\n",
    "    relu_4 = tf.nn.relu(pooling4)\n",
    "    \n",
    "    \"\"\"conv 5\n",
    "    第5层为conv层, 这一层与原始的LeNet-5是一致的\n",
    "    input size: batch size * 5 * 5 * 16\n",
    "    output size: batch size * 1  120\n",
    "    \"\"\"\n",
    "    conv5_filter = tf.Variable(tf.truncated_normal([5, 5, 16, 120]))\n",
    "    conv5_bias = tf.Variable(tf.truncated_normal([120]))\n",
    "    conv5 = tf.nn.conv2d(relu_4, conv5_filter, strides=[1,1,1,1],padding=\"VALID\")\n",
    "    relu_5 = tf.nn.relu(tf.nn.bias_add(conv5, conv5_bias))\n",
    "\n",
    "    relu_5_shaped = tf.reshape(relu_5, [100, 120], name=\"relu_5_reshape\")#relu_5变为 batch size * 120\n",
    "    \n",
    "    \"\"\"fc 6\n",
    "    第6层为fc层,与原始不太一致,因为我只在mnist上进性试验\n",
    "    input size: batch size * 1 * 1 * 120\n",
    "    output size: batch size * 1 * 1 * 10\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "        \n",
    "    fc_weights = tf.Variable(tf.truncated_normal([120,10]))\n",
    "    fc_bias = tf.Variable(tf.truncated_normal([10]))\n",
    "\n",
    "    relu_6 = tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(relu_5_shaped, fc_weights),fc_bias))\n",
    " \n",
    "    \n",
    "    \"\"\"最后的输出单元\n",
    "    softmax分类器\n",
    "    \"\"\"\n",
    "    #logits = tf.nn.softmax(relu_6)\n",
    "    #result = tf.one_hot(tf.argmax(tf.transpose(logits)), 10)\n",
    "    return relu_6\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练过程设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCH = 50000 #迭代次数、\n",
    "LEARNING_RATE = 0.001  # larning rate\n",
    "\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[BATCH_SIZE, 32, 32, 1],name=\"input\")\n",
    "y_predicted = LeNet_inference(x);\n",
    "y_label = tf.placeholder(dtype=tf.float32, shape=[BATCH_SIZE, 10], name=\"label\")\n",
    "\n",
    "\"\"\"\n",
    "loss function and optimization \n",
    "\n",
    "\"\"\"\n",
    "loss_all = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_label, logits=y_predicted)\n",
    "loss = tf.reduce_mean(loss_all)\n",
    "train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "loss_list = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 / 50000;  loss is 2.438265323638916, training is continuing .....\n",
      "iteration: 500 / 50000;  loss is 2.352630376815796, training is continuing .....\n",
      "iteration: 1000 / 50000;  loss is 2.459686517715454, training is continuing .....\n",
      "iteration: 1500 / 50000;  loss is 2.3713276386260986, training is continuing .....\n",
      "iteration: 2000 / 50000;  loss is 2.3488707542419434, training is continuing .....\n",
      "iteration: 2500 / 50000;  loss is 2.349959373474121, training is continuing .....\n",
      "iteration: 3000 / 50000;  loss is 2.272691011428833, training is continuing .....\n",
      "iteration: 3500 / 50000;  loss is 2.296035051345825, training is continuing .....\n",
      "iteration: 4000 / 50000;  loss is 2.2448253631591797, training is continuing .....\n",
      "iteration: 4500 / 50000;  loss is 2.303722620010376, training is continuing .....\n",
      "iteration: 5000 / 50000;  loss is 2.297419548034668, training is continuing .....\n",
      "iteration: 5500 / 50000;  loss is 2.2092669010162354, training is continuing .....\n",
      "iteration: 6000 / 50000;  loss is 2.2241926193237305, training is continuing .....\n",
      "iteration: 6500 / 50000;  loss is 2.226628303527832, training is continuing .....\n",
      "iteration: 7000 / 50000;  loss is 2.268789052963257, training is continuing .....\n",
      "iteration: 7500 / 50000;  loss is 2.149279832839966, training is continuing .....\n",
      "iteration: 8000 / 50000;  loss is 2.2393546104431152, training is continuing .....\n",
      "iteration: 8500 / 50000;  loss is 2.2949657440185547, training is continuing .....\n",
      "iteration: 9000 / 50000;  loss is 2.206831455230713, training is continuing .....\n",
      "iteration: 9500 / 50000;  loss is 2.274634838104248, training is continuing .....\n",
      "iteration: 10000 / 50000;  loss is 2.233384609222412, training is continuing .....\n",
      "iteration: 10500 / 50000;  loss is 2.1775052547454834, training is continuing .....\n",
      "iteration: 11000 / 50000;  loss is 2.2080295085906982, training is continuing .....\n",
      "iteration: 11500 / 50000;  loss is 2.175523042678833, training is continuing .....\n",
      "iteration: 12000 / 50000;  loss is 2.1862659454345703, training is continuing .....\n",
      "iteration: 12500 / 50000;  loss is 2.2029266357421875, training is continuing .....\n",
      "iteration: 13000 / 50000;  loss is 2.184156894683838, training is continuing .....\n",
      "iteration: 13500 / 50000;  loss is 2.205519437789917, training is continuing .....\n",
      "iteration: 14000 / 50000;  loss is 2.1335861682891846, training is continuing .....\n",
      "iteration: 14500 / 50000;  loss is 2.1476285457611084, training is continuing .....\n",
      "iteration: 15000 / 50000;  loss is 2.1710574626922607, training is continuing .....\n",
      "iteration: 15500 / 50000;  loss is 2.1625185012817383, training is continuing .....\n",
      "iteration: 16000 / 50000;  loss is 2.145123243331909, training is continuing .....\n",
      "iteration: 16500 / 50000;  loss is 2.12998366355896, training is continuing .....\n",
      "iteration: 17000 / 50000;  loss is 2.1833722591400146, training is continuing .....\n",
      "iteration: 17500 / 50000;  loss is 2.1910455226898193, training is continuing .....\n",
      "iteration: 18000 / 50000;  loss is 2.1172351837158203, training is continuing .....\n",
      "iteration: 18500 / 50000;  loss is 2.1554336547851562, training is continuing .....\n",
      "iteration: 19000 / 50000;  loss is 2.1381423473358154, training is continuing .....\n",
      "iteration: 19500 / 50000;  loss is 2.163576602935791, training is continuing .....\n",
      "iteration: 20000 / 50000;  loss is 2.1898915767669678, training is continuing .....\n",
      "iteration: 20500 / 50000;  loss is 2.1606040000915527, training is continuing .....\n",
      "iteration: 21000 / 50000;  loss is 2.098418951034546, training is continuing .....\n",
      "iteration: 21500 / 50000;  loss is 2.0976977348327637, training is continuing .....\n",
      "iteration: 22000 / 50000;  loss is 2.098851442337036, training is continuing .....\n",
      "iteration: 22500 / 50000;  loss is 2.1467673778533936, training is continuing .....\n",
      "iteration: 23000 / 50000;  loss is 2.1211678981781006, training is continuing .....\n",
      "iteration: 23500 / 50000;  loss is 2.104548692703247, training is continuing .....\n",
      "iteration: 24000 / 50000;  loss is 2.1237738132476807, training is continuing .....\n",
      "iteration: 24500 / 50000;  loss is 2.1271414756774902, training is continuing .....\n",
      "iteration: 25000 / 50000;  loss is 2.1154236793518066, training is continuing .....\n",
      "iteration: 25500 / 50000;  loss is 2.1160619258880615, training is continuing .....\n",
      "iteration: 26000 / 50000;  loss is 2.1083567142486572, training is continuing .....\n",
      "iteration: 26500 / 50000;  loss is 2.168123245239258, training is continuing .....\n",
      "iteration: 27000 / 50000;  loss is 2.1120615005493164, training is continuing .....\n",
      "iteration: 27500 / 50000;  loss is 2.083469867706299, training is continuing .....\n",
      "iteration: 28000 / 50000;  loss is 2.076378107070923, training is continuing .....\n",
      "iteration: 28500 / 50000;  loss is 2.117069959640503, training is continuing .....\n",
      "iteration: 29000 / 50000;  loss is 2.0766491889953613, training is continuing .....\n",
      "iteration: 29500 / 50000;  loss is 2.0858278274536133, training is continuing .....\n",
      "iteration: 30000 / 50000;  loss is 2.119236946105957, training is continuing .....\n",
      "iteration: 30500 / 50000;  loss is 2.1082146167755127, training is continuing .....\n",
      "iteration: 31000 / 50000;  loss is 2.0439796447753906, training is continuing .....\n",
      "iteration: 31500 / 50000;  loss is 2.11869215965271, training is continuing .....\n",
      "iteration: 32000 / 50000;  loss is 2.114407539367676, training is continuing .....\n",
      "iteration: 32500 / 50000;  loss is 2.0419368743896484, training is continuing .....\n",
      "iteration: 33000 / 50000;  loss is 2.0239973068237305, training is continuing .....\n",
      "iteration: 33500 / 50000;  loss is 2.0818979740142822, training is continuing .....\n",
      "iteration: 34000 / 50000;  loss is 2.115170478820801, training is continuing .....\n",
      "iteration: 34500 / 50000;  loss is 2.053842067718506, training is continuing .....\n",
      "iteration: 35000 / 50000;  loss is 2.0563580989837646, training is continuing .....\n",
      "iteration: 35500 / 50000;  loss is 2.045897960662842, training is continuing .....\n",
      "iteration: 36000 / 50000;  loss is 2.011021852493286, training is continuing .....\n",
      "iteration: 36500 / 50000;  loss is 2.0781660079956055, training is continuing .....\n",
      "iteration: 37000 / 50000;  loss is 2.0658364295959473, training is continuing .....\n",
      "iteration: 37500 / 50000;  loss is 2.0681583881378174, training is continuing .....\n",
      "iteration: 38000 / 50000;  loss is 2.0166592597961426, training is continuing .....\n",
      "iteration: 38500 / 50000;  loss is 2.0873770713806152, training is continuing .....\n",
      "iteration: 39000 / 50000;  loss is 2.038849353790283, training is continuing .....\n",
      "iteration: 39500 / 50000;  loss is 2.047595262527466, training is continuing .....\n",
      "iteration: 40000 / 50000;  loss is 2.012983798980713, training is continuing .....\n",
      "iteration: 40500 / 50000;  loss is 2.0768563747406006, training is continuing .....\n",
      "iteration: 41000 / 50000;  loss is 2.0900020599365234, training is continuing .....\n",
      "iteration: 41500 / 50000;  loss is 2.081472396850586, training is continuing .....\n",
      "iteration: 42000 / 50000;  loss is 2.0262346267700195, training is continuing .....\n",
      "iteration: 42500 / 50000;  loss is 2.0671486854553223, training is continuing .....\n",
      "iteration: 43000 / 50000;  loss is 2.016308546066284, training is continuing .....\n",
      "iteration: 43500 / 50000;  loss is 2.0206520557403564, training is continuing .....\n",
      "iteration: 44000 / 50000;  loss is 2.030179738998413, training is continuing .....\n",
      "iteration: 44500 / 50000;  loss is 2.039816379547119, training is continuing .....\n",
      "iteration: 45000 / 50000;  loss is 1.9919363260269165, training is continuing .....\n",
      "iteration: 45500 / 50000;  loss is 2.0420730113983154, training is continuing .....\n",
      "iteration: 46000 / 50000;  loss is 2.0192975997924805, training is continuing .....\n",
      "iteration: 46500 / 50000;  loss is 2.002084732055664, training is continuing .....\n",
      "iteration: 47000 / 50000;  loss is 2.0001604557037354, training is continuing .....\n",
      "iteration: 47500 / 50000;  loss is 2.008131504058838, training is continuing .....\n",
      "iteration: 48000 / 50000;  loss is 2.0097227096557617, training is continuing .....\n",
      "iteration: 48500 / 50000;  loss is 2.013374090194702, training is continuing .....\n",
      "iteration: 49000 / 50000;  loss is 1.9754815101623535, training is continuing .....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 49500 / 50000;  loss is 2.019651412963867, training is continuing .....\n",
      "training is end\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(NUM_EPOCH):\n",
    "        xs,ys = mnist.train.next_batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "        #reshape the input \n",
    "        xs.shape = (BATCH_SIZE, 28, 28, 1)\n",
    "        xs = np.pad(xs, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant', constant_values= 0)\n",
    "\n",
    "        _, loss_np = sess.run([train_op, loss], feed_dict={x:xs, y_label:ys})\n",
    "        if i% 500 == 0:\n",
    "            loss_list = np.append(loss_list,loss_np )\n",
    "            print(\"iteration: {} / {};  loss is {}, training is continuing .....\".format(i, NUM_EPOCH,loss_np))\n",
    "    print(\"training is end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRBCKOyGERBCKOyGERJD/BQKeCvoR/ZH2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c14201b978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_list, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义测试模块,统计现有模型的准确率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集说明:  \n",
    "training dataset : 55000;   test dataset: 10000;  validation dataset: 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = mnist.test.images\n",
    "test_label = mnist.test.labels\n",
    "test_x.shape = (mnist.test.num_examples, 28, 28, 1)\n",
    "test_shaped = np.pad(test_x, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant', constant_values = 0);\n",
    "\n",
    "numCorrect = 0;\n",
    "for i_subTest in range(100):\n",
    "    subTest = test_shaped[i_subTest*BATCH_SIZE:(i_subTest+1)*BATCH_SIZE,: ,:, :]\n",
    "    subTest_label = test_label[i_subTest*BATCH_SIZE:(i_subTest+1)*BATCH_SIZE, :]\n",
    "    pred = tf.nn.softmax(LeNet_inference(subTest), axis=0) #prediction from neural networks\n",
    "    assert pred.shape == subTest_label.shape # checkout of predictions' shape\n",
    "    ture_false = tf.equal(tf.argmax(pred, axis=1), tf.argmax(subTest_label, axis=1))\n",
    "    numCorrect += tf.reduce_sum(tf.cast(ture_false, dtype=tf.int32))\n",
    "                                \n",
    "                                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:   \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"正确识别率:\", sess.run(numCorrect)/mnist.test.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "143px",
    "width": "234px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "194px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
