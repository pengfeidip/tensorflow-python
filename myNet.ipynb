{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import Ipynb_importer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-05ab45eebe5b>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From G:\\Anaconda\\installCatalog\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From G:\\Anaconda\\installCatalog\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./input_data\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From G:\\Anaconda\\installCatalog\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ./input_data\\train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From G:\\Anaconda\\installCatalog\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ./input_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ./input_data\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From G:\\Anaconda\\installCatalog\\envs\\tensorflow_cpu\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./input_data', one_hot=True, validation_size=500)\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# network 设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "第一层为卷积层, 卷积层_1的参数  \n",
    "input:  filter: \n",
    "        size: 5*5;  number: 6  channel:  1  \n",
    "output: feature map: \n",
    "        size 28*28, number 6; channel :1\n",
    "\"\"\"\n",
    "CONV1_FILTER_SIZE = 5\n",
    "CONV1_FILTER_CHANNEL = 1\n",
    "CONV1_FILTER_NUM = 6\n",
    "\n",
    "\"\"\"\n",
    "第三层为卷积层, 卷积层_3的参数\n",
    "input: filter:\n",
    "       size 5 * 5   number 16 channel: 1\n",
    "output: feature map\n",
    "       size 10 * 10 number 16  channel: 1 \n",
    "       \n",
    "\"\"\"\n",
    "CONV3_FILTER_SIZE = 5\n",
    "CONV3_FILTER_CHANNEL = 1\n",
    "CONV3_FILTER_NUM = 16\n",
    "\n",
    "\"\"\"\n",
    "第五层是一个卷积层, \n",
    "input: filter:\n",
    "        size 5 *5    number 120   channel: 1\n",
    "output: feature map\n",
    "        size 1    number 120  channel: 1\n",
    "\"\"\"\n",
    "CONV5_FILTER_SIZE = 5\n",
    "CONV5_FILTER_CHANNEL = 1\n",
    "CONV5_FILTER_NUM = 120\n",
    "\n",
    "\"\"\"\n",
    "设置一些训练参数\n",
    "\"\"\"\n",
    "BATCH_SIZE = 100;\n",
    "\n",
    "# LeNet_inference\n",
    "\n",
    "def LeNet_inference(inTensor):\n",
    "    \n",
    "    \"\"\"conv 1.  第一层为卷积层,有6个filter\n",
    "    input size: 32 * 32 * 1\n",
    "    output size: 28 * 28 * 6\n",
    "    \"\"\"\n",
    "    conv1_filter = tf.Variable(tf.truncated_normal([CONV1_FILTER_SIZE,CONV1_FILTER_SIZE,CONV1_FILTER_CHANNEL, CONV1_FILTER_NUM],dtype=tf.float32))\n",
    "    conv1 = tf.nn.conv2d(inTensor, conv1_filter, strides=[1, 1, 1, 1], padding='VALID')\n",
    "    conv1_bias = tf.Variable(tf.truncated_normal([CONV1_FILTER_NUM], dtype=tf.float32))\n",
    "    relu_1 = tf.nn.relu(tf.nn.bias_add(conv1, conv1_bias))\n",
    "    \n",
    "    \"\"\"pooling 2.  \n",
    "     第2层为pooling层,与原始LetNet不同.原始的LeNet的pooling层有两个可训练参数\n",
    "     input size: 28 * 28 * 6\n",
    "     output size: 14 * 14 * 6\n",
    "    \"\"\"\n",
    "    pooling2 = tf.nn.avg_pool(relu_1, ksize=[1,2,2,1], strides=[1, 2, 2 ,1], padding=\"VALID\")\n",
    "    relu_2 = tf.nn.relu(pooling2)\n",
    "    \n",
    "    \n",
    "    \"\"\" conv 3  \n",
    "    第3层为卷积层,共16个filter. 与原始LeNet不同,原始的LeNet是部分连接\n",
    "    这里为了方便是,整体连接,详细情况见论文希捷\n",
    "    input size 14 * 14 * 6\n",
    "    out putsize 10 * 10 *16\n",
    "    \"\"\"\n",
    "    conv3_filter = tf.Variable(tf.truncated_normal([5, 5, 6, 16]))\n",
    "    conv3_bias = tf.Variable(tf.truncated_normal([16]))\n",
    "    conv3 = tf.nn.conv2d(relu_2, conv3_filter, strides=[1,1,1,1],padding='VALID')\n",
    "    relu_3 =tf.nn.relu(tf.nn.bias_add(conv3, conv3_bias))\n",
    "    \n",
    "    \n",
    "    \"\"\"pooling 4  \n",
    "    第4层为pooling层, 与pooling 2 相同\n",
    "    input size:  10 * 10 * 16\n",
    "    output size: 5 * 5 * 16\n",
    "    \"\"\"\n",
    "    pooling4 = tf.nn.avg_pool(relu_3, ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"VALID\")\n",
    "    relu_4 = tf.nn.relu(pooling4)\n",
    "    \n",
    "    \"\"\"conv 5\n",
    "    第5层为conv层, 这一层与原始的LeNet-5是一致的\n",
    "    input size: batch size * 5 * 5 * 16\n",
    "    output size: batch size * 1  120\n",
    "    \"\"\"\n",
    "    conv5_filter = tf.Variable(tf.truncated_normal([5, 5, 16, 120]))\n",
    "    conv5_bias = tf.Variable(tf.truncated_normal([120]))\n",
    "    conv5 = tf.nn.conv2d(relu_4, conv5_filter, strides=[1,1,1,1],padding=\"VALID\")\n",
    "    relu_5 = tf.nn.relu(tf.nn.bias_add(conv5, conv5_bias))\n",
    "\n",
    "    relu_5_shaped = tf.reshape(relu_5, [100, 120], name=\"relu_5_reshape\")#relu_5变为 batch size * 120\n",
    "    \n",
    "    \"\"\"fc 6\n",
    "    第6层为fc层,与原始不太一致,因为我只在mnist上进性试验\n",
    "    input size: batch size * 1 * 1 * 120\n",
    "    output size: batch size * 1 * 1 * 10\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "        \n",
    "    fc_weights = tf.Variable(tf.truncated_normal([120,10]))\n",
    "    fc_bias = tf.Variable(tf.truncated_normal([10]))\n",
    "\n",
    "    relu_6 = tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(relu_5_shaped, fc_weights),fc_bias))\n",
    " \n",
    "    \n",
    "    \"\"\"最后的输出单元\n",
    "    softmax分类器\n",
    "    \"\"\"\n",
    "    #logits = tf.nn.softmax(relu_6)\n",
    "    #result = tf.one_hot(tf.argmax(tf.transpose(logits)), 10)\n",
    "    return relu_6\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练过程设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCH = 10000 #迭代次数、\n",
    "LEARNING_RATE = 0.001  # larning rate\n",
    "\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[BATCH_SIZE, 32, 32, 1],name=\"input\")\n",
    "y_predicted = LeNet_inference(x);\n",
    "y_label = tf.placeholder(dtype=tf.float32, shape=[BATCH_SIZE, 10], name=\"label\")\n",
    "\n",
    "\"\"\"\n",
    "loss function and optimization \n",
    "\n",
    "\"\"\"\n",
    "loss_all = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_label, logits=y_predicted)\n",
    "loss = tf.reduce_mean(loss_all)\n",
    "train_op = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "loss_list = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0 / 20000;  loss is 2.4801745414733887\n",
      "iteration: 500 / 20000;  loss is 2.427987575531006\n",
      "iteration: 1000 / 20000;  loss is 2.449130058288574\n",
      "iteration: 1500 / 20000;  loss is 2.3882336616516113\n",
      "iteration: 2000 / 20000;  loss is 2.4539411067962646\n",
      "iteration: 2500 / 20000;  loss is 2.341538906097412\n",
      "iteration: 3000 / 20000;  loss is 2.436142683029175\n",
      "iteration: 3500 / 20000;  loss is 2.426692485809326\n",
      "iteration: 4000 / 20000;  loss is 2.358266592025757\n",
      "iteration: 4500 / 20000;  loss is 2.380737543106079\n",
      "iteration: 5000 / 20000;  loss is 2.315382957458496\n",
      "iteration: 5500 / 20000;  loss is 2.399726629257202\n",
      "iteration: 6000 / 20000;  loss is 2.385697841644287\n",
      "iteration: 6500 / 20000;  loss is 2.2714219093322754\n",
      "iteration: 7000 / 20000;  loss is 2.3409249782562256\n",
      "iteration: 7500 / 20000;  loss is 2.3881030082702637\n",
      "iteration: 8000 / 20000;  loss is 2.330941677093506\n",
      "iteration: 8500 / 20000;  loss is 2.308046579360962\n",
      "iteration: 9000 / 20000;  loss is 2.246957302093506\n",
      "iteration: 9500 / 20000;  loss is 2.282299518585205\n",
      "iteration: 10000 / 20000;  loss is 2.246610403060913\n",
      "iteration: 10500 / 20000;  loss is 2.2129926681518555\n",
      "iteration: 11000 / 20000;  loss is 2.271834373474121\n",
      "iteration: 11500 / 20000;  loss is 2.2954277992248535\n",
      "iteration: 12000 / 20000;  loss is 2.2368385791778564\n",
      "iteration: 12500 / 20000;  loss is 2.2612829208374023\n",
      "iteration: 13000 / 20000;  loss is 2.2600317001342773\n",
      "iteration: 13500 / 20000;  loss is 2.2639639377593994\n",
      "iteration: 14000 / 20000;  loss is 2.186464309692383\n",
      "iteration: 14500 / 20000;  loss is 2.277620792388916\n",
      "iteration: 15000 / 20000;  loss is 2.2818799018859863\n",
      "iteration: 15500 / 20000;  loss is 2.1736035346984863\n",
      "iteration: 16000 / 20000;  loss is 2.198869228363037\n",
      "iteration: 16500 / 20000;  loss is 2.161160469055176\n",
      "iteration: 17000 / 20000;  loss is 2.2022523880004883\n",
      "iteration: 17500 / 20000;  loss is 2.244422197341919\n",
      "iteration: 18000 / 20000;  loss is 2.281593084335327\n",
      "iteration: 18500 / 20000;  loss is 2.1710805892944336\n",
      "iteration: 19000 / 20000;  loss is 2.229653835296631\n",
      "iteration: 19500 / 20000;  loss is 2.193704128265381\n",
      "training is end\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(20000):\n",
    "        xs,ys = mnist.train.next_batch(batch_size=BATCH_SIZE)\n",
    "\n",
    "        #reshape the input \n",
    "        xs.shape = (BATCH_SIZE, 28, 28, 1)\n",
    "        xs = np.pad(xs, ((0, 0), (2, 2), (2, 2), (0, 0)), 'constant', constant_values= 0)\n",
    "\n",
    "        _, loss_np = sess.run([train_op, loss], feed_dict={x:xs, y_label:ys})\n",
    "        if i% 500 == 0:\n",
    "            loss_list = np.append(loss_list,loss_np )\n",
    "            print(\"iteration: {} / 20000;  loss is {}\".format(i, loss_np))\n",
    "    print(\"training is end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "143px",
    "width": "234px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "194px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
